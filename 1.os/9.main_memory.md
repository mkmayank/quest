* Memory consists of a large array of bytes, each with its own address
* CPU fetches instructions from memory according to the value of the program counter
    * These instructions may cause additional loading from
and storing to specific memory addresses

A typical instruction-execution cycle
* first fetches an instruction from memory
* instruction is then decoded and may cause operands to be fetched from memory
* After the instruction has been executed on the operands, results may be stored back in memory
* memory unit sees only a stream of memory addresses
    * it does not know how they are generated
        * by the instruction counter, indexing, indirection, literal addresses, and so on
    * or what they are for
        * instructions or data

---
* CPU can directly access only these general pupose storage:
    * main memory and
    * registers built into each processing core are the only
* machine instructions take memory addresses as arguments and not disk addresses
* any data being used by the instructions must be in one of these direct-access storage devices

<br/>

* registers are fast accessible , within one cycle of CPU clock
* completing a memory access may take many cycles of the CPU clock
    * problem:
        * cpu has to stalled as no data is there for time being
    * solution:
        * add fast memory between the CPU and main memory, typically on the CPU chip for fast access
            * cache
    * to manage a cache built into the CPU , the hardware automatically speeds up memory access without any OS control
    * during a memory stall, a multithreaded core can switch from the stalled hardware thread to another hardware thread

<br/>

to protect user process from one another
* protection must be provided by hardware
    * OS doesn't intervene between CPU and its memory accesses
        * because of performance penality
* this can be implemented in various ways by hardware
* e.g.
    * each process should have separate memory space
    * define strict range of legal addresses a process may access
    * this protection can be provied by using two registers
        * base register
            * holds the smallest legal physical memory address
        * limit
            * specifies the size of the range
        * e.g.
            * base register holds 300040 and
            * limit register is 120900, then
            * program can legally access all addresses from 300040 through 420939
    * cpu hardware compares every address generated in user mode with the registers
    * any illegal attempt results in a trap to OS
    * base and limit registers can only be loaded by OS (cpu scheduler/dispatcher) via priviliged instruction
* OS, executin in kernel mode is given unrestricted access to both OS and users' memory
    * this allows OS to load and unload users' program  into users' memory

---

* program resides on disk, to run, program must be brough into memory and placed within the context of a process
* instrunctions and data need to be assigned a memory address, so that it can used by CPU

user program goes through several steps (some may be optional) before being executed
* addresses in the source program are generally symbolic
    * e.g. `variable count`
* compiler typically binds these symbolic addresses to relocatable addresses
    * e.g. `14 bytes from the beginning of this module`)
* linker or loader in turn binds the relocatable addresses to absolute addresses
    * e.g. `74014`
* each binding is a mapping from one address space to another

binding of instructions and data to memory addresses can be done at any step along the way:
* Compile time.
    * only if we know where will the process will reside in memory
    * not practical, as thre will be a need to compile at every time starting location changes
* Load time
* Execution time
    * if process can be moved during its execution from one memory segment to another
    * binding must be delayed until run time

---

|||
|-|-|
| logical address  | address generated by the CPU |
| physical address | address seen by the memory unit |

* Binding addresses at either compile or load time generates identical logical
and physical addresses
* execution-time address-binding scheme results in differing logical and physical addresses
    * logical addresses are reffered as virtual address

<br/>

* run-time mapping from virtual to physical addresses is done by a hardware device called **memory-management unit ( MMU )**

mapping can be done by various ways
* example
    * using a relocation register (similar as above example base limit)
    * value in the relocation register is added to every address generated by a user process at the time the address is sent to memory
    * e.g.
        * register value at 12000
        * attempt by user to address location 0 is dynamically reloacted to location 12000
        * or access to 494 is mapped to 12494
    * user program never accesses the real physical addresses
    * user program deals with logical addresses
    * final location of a referenced memory address is not determined until the reference is made
    * logical addresses (range 0 to max)
    * physical addresses (R + 0 to R + max,  for a base value R)

---

problem:
* entire program and all data of a process need to be in physical memory fo rthe process to execute
* size of process is limited to the size fo physical memory

solution:
* **dynamic loading**
    * routine is not loaded until it is needed
    * dynamic loading does not require special support from the OS
    * it is the responsibility of the users to design their programs to take advantage of such a method
    * OS may help programmer by providing library routines to implement dynamic loading

* **Dynamic Linking and Shared Libraries**
    * **Dynamically linked libraries ( DLLs )**
    * system libraries that are linked to user programs when the programs are run
    * here linking, rather than loading, is postponed until execution time
    * another advantage, shared libraries can be shared among mutiple process
    * this is done via OS
---

Contiguous Memory Allocation
* memory is usually divided into two partitions
    * one for the OS
    * one for the user processes
* OS can be placed in either low memory addresses or high memory addresses
* Linux place OS in high memory
* In contiguous memory allocation, each process is contained in a single section of memory that is contiguous to the section containing the next process

simplest methods of allocating memory
* is to assign processes to variably sized partitions in memory
    * where each partition may contain exactly one process
* OS keeps a table indicating which parts of memory are available and which are occupied
* OS calculates memory requirements of each new process and the amount of available memory space in determining which processes are allocated memory
* Problem:
    * if no memory avaialble to satisfy demand
* Solution:
    * simply reject the process and show error message or
    * place process into wait queue
* memory blocks available comprises a set of hold of various sizes scattered throughout memory
* when a process arrives and needs memory
    * OS searches the set for a hole large enough for the process
    * if hole is large, OS split it into  two parts
        * allocate one part to process
        * other is returned to set of holes
* procedure to satisfy a request of size n from a list of free holes
    * first-fit
        * allocated the first hole that is big enough
    * best-fit
        * allocates the smallest hole that is big enough
    * worst-fit
        * allocates the largest hole
* Problem:
    * memory space is fragmented into a large number of small holes

**External Fragmentation**
* when there is enough total memory space to satisfy a request but available space are not contigous

**Internal Fragmentation**
* unused memory that is internal to a partition

Solution to external fragmentation
* compaction
    * shuffle the memory contents so as to place all free memory together in one large block
    * no always possible and is expensive
* **paging**
    * permits a process’s physical address space to be noncontiguous
    * common memory-management technique for OS
---

**Paging**
* permits a process’s physical address space to be noncontiguous
* avoids external fragmentation and the associated need for compaction
* common way for memory allocation in OS (various forms of paging)
* paging is implemented through cooperation between the OS and hardware

basic method for implementing paging
* breaking physical memory into fixed-sized blocks called **frames**
* breaking logical memory into blocks of the same size called **pages**
* address from CPU is divided into two parts
    * page number
    * page offset
* page number is used as an index into a per-process page table
* page table contains the base address of each frame in physical memory
    * and the offset is the location in the frame being referenced

steps taken by MMU to translate logical address generated by CPU to physical address
* extract page number p and use it as an index into the page table
* extract corresponding frame number f from the page table
* replace the page number p in the logical address with the frame number f
* as the offset d does not change, it is not replaced, and the frame number and offset now comprise the physical address

<br/>

* pages table also has bits
* like valid/invlaid bits
    * to denote whether page is valid or not

<br/>

* page size (like the frame size) is defined by the hardware
* size of a page is a power of 2, depending on the computer architecture
* e.g.
    * size of the logical address space is 2^m
    * page size is 2^n bytes
        * high order m-n bits of alogical address designate the page number
        * n low-order bits designate the page offset
    * (simple math :))

<br/>

* no external fragmentation
* last frame may have internal fragmentaion
* Linux supports two page sizes
    * a default page size (typically 4 KB )
    * an architecture dependent larger page size called **huge pages**

```bash
$ getconf PAGESIZE
4096
```

* OS examined process's size in pages
* for n pages, n frames to be available in memory
* if available n frames are allocated to process
* first page is loaded into one of allocated frame
    * frame number is put in the process's page table
* then next page is loaded and so on

<br/>

* program views memory as one single space containing only this one program
* but program is scattered thoroughout physical memory

<br/>

* user process by definition is unable to access memory it does not own
* It has no way of addressing memory outside of its page table
    * as table includes only those pages that the process owns

<br/>

OS keep allocation details of physical memory in single, system wide table callled **frrame table**
* which frames are allocated
* which frames are available
* how many total frames there are, and so on

OS maintains a copy of the page table for each process, just as it maintains a copy of the instruction counter and register contents
* it is used to translate logical addresses to physical addresses whenever the OS must map a logical address to a physical address manually
* it is also used by the CPU dispatcher to define the hardware page table when a process is to be allocated to the CPU
* Paging therefore increases the context-switch time

---

pointer to page table is stored with other register values(like instruction pointer) in process control block of each process

Instead of page table in hardware registers, page table is kept in main memory and **page-table base register (PTBR)** points to page table
* otherwise context-switch time would have increased substantially
* changing page tables requires changing only this one register
    * substantially reducing context-switch time

---
Problem:
* storing page table in main memory can yield faster context switches
* but it may result in slower memory access times
    * as two memory accesses are needed to access data
        * one for the page-table entry and
        * one for the actual data

Solution:
* **translation look-aside buffer ( TLB )**
    * a special, small, fast-lookup hardware cache
    * associative, high-speed memory.
    * compare ask item with all keys simultaneously
    * a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty ( ~10 ns)
    * CPUs may have mutiple TLBs
    * TLB contains only a few of the page-table entries
        * in case of miss, frame number is obtained from memory
            * and added to TLB
                * replacement policies may use
                    * LRU (least recently used)
                    * round-robin
                    * random, etc
    * some TLBs allow certain entries to be wired down
        * kernel code can be wired down
    * some TLBs store **address-space identifier (ASIDs)** in each TLB entry
        * to uniquely identify each process
        * it allows to hold mutiple processes's entries simultaneously
    * it is hardware feature and is of little concern with OS

---

Structure of page table ???:
* Hierarchical paging
    * pagin table is itself paged
        * (mutiple times like 2 level, 3 leve, 4 level, so on)
* Hashed Page Tables
* Inverted Page Tables
* Oracle SPARC Solaris

---

**Swapping**
* a process, or a portion of a process, can be swapped temporarily out of memory to a backing store(disk) and then brought back into memory for continued execution
* Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system

Standard Swapping
* moving entire processes between main memory and a backing store
* Idle or mostly idle processes are good candidates for swapping
* was used in traditional UNIX, no longer used in now OS

Swapping with Paging
* instead of whole process, pages are swapped

the term swapping now generally refers to standard swapping and paging refers to swapping with paging
* **page out** - operation moves a page from memory to the backing store
* reverse process is known as a **page in**

Swapping on Mobile Systems
* mobile OS supports no swapping
    * due to use of limited flash memory
    * limited number of writes that flash memory can tolerate
    * poor throughput between main memory and flash memory
* instead of swapping
    * applications are asked to free up the memory
        * or may be terminated
    * read only data may be removed and can be loaded later to free up main memory
* android terminates application and write application state to flash memory in case memory is low
---

* Example: Intel 32- and 64-bit Architectures ???
* Example: ARMv8 Architecture ???