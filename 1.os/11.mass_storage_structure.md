The bulk of secondary storage for modern computers is provided by
* hard disk drives ( HDDs ) and
* nonvolatile memory ( NVM ) devices

---

Hard Disk Drives
* each disk platter has a flat circular shape, like a CD
    * common platter diameters range from 1.8 to 3.5 inches
* two surfaces of a platter are covered with a magnetic material
* a read-write head “flies” just above each surface of every platter
* heads are attached to a disk arm that moves all the heads as a unit
* surface of a platter is logically divided into circular tracks, which are subdivided into sectors
* set of tracks at a given arm position make up a cylinder
* there may be thousands of concentric cylinders in a disk drive
    * each track may contain hundreds of sector
* each sector has a fixed size and is the smallest unit of transfer
* sector size was commonly 512 bytes until around 2010 and then 4KB sectors
* disk drive motor spins it at high speed
    * rotate 60 to 250 times per second
    * (rotaion per minute RPM ) spin at 5,400, 7,200, 10,000, and 15,000 RPM
* rotation speed relates to transfer rates
    * transfer rate is the rate at which data flow between the drive and the computer
* positioning time, or random-access time, consists of two parts
    * time necessary to move the disk arm to the desired cylinder, called the **seek time**
    * time necessary for the desired sector to rotate to the disk head, called the **rotational latency**
* increase performance by having DRAM buffers in the drive controller
* disk head flies on an extremely thin cushion (measured in microns) of air or another gas, such as helium, and there is a danger that the head will make contact with the disk surface
    * disk platters are coated with a thin protective layer
        * head will sometimes damage the magnetic surface
        * this accident is called a head crash
        * head crash normally cannot be repaired
* HDDs are sealed units

---

Nonvolatile Memory Devices
* electrical rather than mechanical
* NVM can be like
    * composed of a controller and flash NAND die semiconductor chips or
    * DRAM with battery backing
    * semiconductor technology like 3D XPoint
* e.g. SSD ( solid-state disk), pen drive, ...
* NVM devices can be more reliable than HDDs
    * because they have no moving parts and
    * can be faster because they have no seek time or rotational latency
* consumes less power
* more expensive per megabyte than traditional hard disks
* NAND semiconductors characteristics
    * they can be read and written in a page increment (similar to a sector)
    * but data cannot be overwritten rather, the NAND cells have to be erased first
    * read is fastest, write is faster but erase is slowest
    * NAND semiconductors also deteriorate with every erase cycle
        * after x program-erase cycles, the cells no longer retain data
    * Because of the write wear, and because there are no moving parts
        * NAND NVM lifespan is not measured in years but in Drive Writes Per Day (DWPD)
            * measure how many times the drive capacity can be written per day before the drive fails
            * e.g.
                * 1 TB NAND drive with a 5 DWPD rating
                    * is expected to have 5 TB per day written to it for the warranty period without failure

---

* secondary storage device is attached to a computer by the system bus or an I/O bus
* several kinds of buses are available
    * advanced technology attachment ( ATA ),
    * serial ATA ( SATA )
    * eSATA
    * serial attached SCSI ( SAS )
    * universal serial bus ( USB )
    * fibr channel ( FC )
* Because NVM devices are much faster than HDD s, the industry created a special, fast interface for NVM devices called NVM express ( NVMe )
    * NVMe directly connects the device to the system PCI bus
        * increasing throughput and
        * decreasing latency compared with other connection methods

* data transfers on a bus are carried out by special electronic processors called controllers (or host-bus adapters ( HBA ))
    * host controller is the controller at the computer end of the bus
    * device controller is built into each storage device
* to perform a mass storage I/O operation
    * computer places a command into the host controller
        * typically using memory-mapped I/O ports
    * host controller then sends the command via messages to the device controller
    * controller operates the drive hardware to carry out the command
        * device controllers usually have a built-in cache
* data transfer at the drive happens between the cache and the storage media
* data transfer to the host, at fast electronic speeds, occurs between the cache host DRAM via DMA ???

---

HDD scheduling

OS is responsible to use HDDs efficiently by
* minimizing access time
    * seek time
        * time for the device arm to move the heads to the cylinder containing the desired sector
    * rotational latency
        additional time for the platter to rotate the desired sector to the head
* maximizing data transfer bandwidth

<br/>

for I/O
* if desired drive and controller are available
    * request can be serviced immediately
* if drive or controller is busy
    * new requests for service will be placed in the queue of pending requests for that drive
* This waiting queue cna be optimized to provied better performance

<br/>

* In the past, HDD interfaces required that the host specify which track and which head to use, and much effort was spent on disk scheduling algorithms
* newer drives do not expose these controls to the host but also map LBA to physical addresses under drive control
* current goals of disk scheduling include
    * fairness
    * timeliness
    * optimizations
        * such as bunching reads or writes that appear in sequence, as drives perform best with sequential I/O
* absolute knowledge of head location and physical block/cylinder locations is not possible on modern drives

FCFS Scheduling
* first-come, first-served
* e.g.
    * disk queue with requests for I/O to blocks on cylinders
        * 98, 183, 37, 122, 14, 124, 65, 67
    * it involves lot of head movement
    * e.g.
        * wild swing from 122 to 14 and then back to 124 illustrates the problem
with this schedule

SCAN Scheduling
* disk arm starts at one end of the disk and moves toward the other end
    * servicing requests as it reaches each cylinder, until it gets to the other end of the disk
    * at the other end, the direction of head movement is reversed, and servicing continues
* head continuously scans back and forth across the disk
* e.g.
    * queue 98, 183, 37, 122, 14, 124, 65, 67
    * initial head is 53 and moving towards 0
        * head will service 37 and then 14
        * at cylinder 0 arm will reverse and will move in reverse
            * and wil service 65, 67, 98, 122, 124, 183

C-SCAN Scheduling
* Circular SCAN ( C-SCAN )
* variant of SCAN designed to provide a more uniform wait time
* when the head reaches the other end, it immediately returns to the beginning of the disk without servicing any requests on the return trip
* e.g.
    * queue 98, 183, 37, 122, 14, 124, 65, 67
    * initial head is 53 and moving towards 199
    * requests will be served as
        * 65, 67, 98, 122, 124, 183
        * on reaching end heas will be places at 0
            and will service 14, 37

---

* SCAN and C-SCAN perform better for systems that place a heavy load on the disk, because they are less likely to cause a starvation problem
* There can still be starvation though, which drove Linux to create the deadline scheduler
    * This scheduler maintains separate
        * read and
        * write queues
    * gives reads priority because processes are more likely to block on read than write
    * queues are sorted in LBA order, essentially implementing C-SCAN
    * All I/O requests are sent in a batch in this LBA order
    * Deadline keeps four queues:
        * two read and two write
        * one sorted by LBA and the other by FCFS
    * It checks after each batch to see if there are requests in the FCFS queues older than a configured age (by default,500 ms)
    * If so, the LBA queue (read or write) containing that request is selected for the next batch of I/O

* another scheduler
    * NOOP is preferred for CPU -bound systems using fast storage such as NVM devices
    * Completely Fair Queueing scheduler ( CFQ ) is the default for SATA drives
        * maintains 3 queues (with insertion sort to keep them sorted in LBA order)
            * real time
            * best effort (the default)
            * idle
        * each has exclusive priority over the others, in that order, with starvation possible
        * it uses historical data, anticipating if a process will likely issue more I/O requests soon
        * if it so determines, it idles waiting for the new I/O
            * ignoring other queued requests
            * to minimize seek time, assuming locality of reference of storage I/O requests, per process

---

NVM scheduling

* NVM devices do not contain moving disk heads
    * commonly use a simple FCFS policy
* time required to service
    * reads is uniform
    * write service time is not uniform
        * some SSD schedulers have exploited this property and merge only adjacent write requests, servicing all read requests in FCFS order

**IOPS** I/O operations per second
* NVM device near its end of life due to many erase cycles generally has much worse performance than a new device
* One way to improve the lifespan and performance of NVM devices over
time is to have the file system inform the device when files are deleted, so that
the device can erase the blocks those files were stored on

impact of NVM garbage collection on performance
* e.g.
    * an NVM device under random read and write load
    * all blocks have been written to, but there is free space available
    * Garbage collection must occur to reclaim space taken by invalid data
    * That means that a write might cause a read of one or more pages
        * a write of the good data in those pages to overprovisioning space
        * an erase of the all-invalid-data block
        * the placement of that block into overprovisioning space
    * In summary, one write request eventually causes
        * a page write (the data)
        * one or more page reads (by garbage collection), and
        * one or more page writes (of good data from the garbage-collected blocks)
* creation of I/O requests not by applications but by the NVM device doing garbage collection and space management is called **write amplificatio** and can greatly impact the write performance of the device
* In the worst case, several extra I/Os are triggered with each write request.

---

* new storage device is a blank slate
    * just a platter of a magnetic recording material or
    * a set of uninitialized semiconductor storage cells
* Before a storage device can store data
    * it must be divided into sectors that the controller can read and write
    * NVM pages must be initialized and the FTL created
* **low-level formatting, or physical formatting** ???
    * fills the device with a special data structure for each storage location
    * data structure for a sector or page typically consists of a header, a data area, and a trailer
    * header and trailer contain information used by the controller, such as a sector/page number and an error detection or correction code
* most drives are low-level-formatted at the factory as a part of the manufacturing process

<br/>

OS needs to record its own data structures on the device before it can be used
1. **partition** the device into one or more groups of blocks or pages
    * OS can treat each partition as though it were a separate device
    * partition information is written in a fixed format at a fixed location on the storage device
2. **volume creation and management**
    * this step may be implicit
        * when a file system is placed directly within a partition
    * can be explicit
        * when multiple partitions or devices to be used together as a RAID set ?
3. **logical formatting**
    * creation of a file system
    * these data structures may include maps of free and allocated space and an initial empty directory

* partition information also indicates if a partition contains a bootable file system (containing the operating system)
    * partition labeled for boot is used to establish the root of the file system
* to increase efficiency, most file systems group blocks together into larger chunks, frequently called **clusters** ???
    * Device I/O is done via blocks, but file system I/O is done via clusters
* file systems try to group file contents near its metadata as well, reducing HDD head seeks when operating on a file
* OS allows to use a partition as a large sequential array of logical blocks, without any file-system data structures
    * This array is sometimes called the **raw disk**, and I/O to this array is termed **raw I/O**
    * Raw I/O bypasses all the file-system services, such as the buffer cache, file locking, prefetching, space allocation, file names, and directories

---

Bad Blocks

* Due to moving parts and small tolerances
    * disk head flies just above the disk surface
* they are prone to failure
* most disks come from the factory with bad blocks

Handling of bad blocks
* On older disks, bad blocks are handled manually
    * scan the disk to find bad blocks while the disk is being formatted
        * any bad blocks that are discovered are flagged as unusable so that the file system does not allocate them
    * if blocks go bad during normal operation, a special program
        * (such as the Linux badblocks command)
        * must be run manually to search for the bad blocks and to lock them away
* Data that resided on the bad blocks usually are lost
* More sophisticated disks are smarter about bad-block recovery
    * controller maintains a list of bad blocks on the disk
    * list is initialized during the low-level formatting at the factory and
    * is updated over the life of the disk
* Low-level formatting also sets aside spare sectors not visible to the OS
    * controller can be told to replace each bad sector logically with one of the spare sectors
    * this scheme is known as **sector sparing or forwarding**

NVM devices
* also have bits, bytes, and even pages that either are nonfunctional at manufacturing time or go bad over time
* Management of those faulty areas is simpler than for HDDs because there is no seek time performance loss to be avoided
* either multiple pages can be set aside and used as replacement locations, or space from the over-provisioning area can be used
* either way, the controller maintains a table of bad pages and never sets those pages as available to write to, so they are never accessed

---

Storage Attachment
* Host-Attached Storage
    * accessed through local I/O ports
    * **SATA**
    * **fiber channek (FC)**
* Network-Attached Storage ( **NAS** )
    * access to storage across a network
    * NAS device can be either
        * a special-purpose storage system or
        * a general computer system that provides its storage to other hosts across the network
    * Clients access network-attached storage via a **remote-procedure-call (RPC)** interface such as NFS for UNIX and Linux systems
    * **iSCSI** is the latest network-attached storage protocol
* Cloud Storage
    * access to storage across a network
    * storage is accessed over the Internet or another WAN to a remote data center that provides storage

<br/>

* NAS is accessed as just another file system if the CIFS or NFS protocols are used, or as a raw block device if the iSCSI protocol is used
* cloud storage is API based and programs use the APIs to access the storage

<br/>

**storage-area network ( SAN )**
* private network (using storage protocols rather than networking protocols connecting servers and storage units
* power of a SAN lies in its flexibility
* multiple hosts and multiple storage arrays can attach to the same SAN , and storage can be dynamically allocated to hosts
* storage arrays can be RAID protected or unprotected drives **(Just a Bunch of Disks ( JBOD ))**
* SAN switch allows or prohibits access between the hosts and the storage
* SAN connectivity is over short distances and typically has no routing, so a NAS can
have many more connected hosts than a SAN
* FC is the most common SAN interconnect, although the simplicity of iSCSI is increasing its use
* another SAN interconnect is **InfiniBand** ( IB ) a special purpose bus architecture that provides hardware and software support for high-speed interconnection networks for servers and storage units

---

trasnfer rate can be improved by sripping data across the drives (multiple drives)
* **bit-level stripping**
    * consists of splitting the bits of each byte across multiple drives
* **block-level striping**
    * blocks of a file are striped across multiple drives
* Block-level striping is the only commonly available striping

---

**mirroring**
* duplicate every drive

---

**redundant arrays of independent disks ( RAIDs )** ???

**RAID level 0**
* refers to drive arrays with striping at the level of blocks
    * without parity information, redundancy, or fault tolerance

**RAID level 1**
* refers to drive mirroring
    * no parity, striping

**RAID level 4** ???

**RAID level 5** ???

**RAID level 6** ???

**Multidimensional RAID level 6** ???

**RAID levels 0 + 1 and 1 + 0** ???

---

**ZFS** ???

---

**Object Storage**
* **Hadoop**