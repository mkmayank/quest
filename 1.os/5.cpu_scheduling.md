* processes alternate between two states:
    * CPU execution
    * I/O wait
* process execution is :
    * CPU burst -> I/O burst -> CPU burst -> .... -> terminate request

**non-preemptive scheduling**
* CPU is allocated to the process till it terminates or switches to waiting state

**preemptive scheduling**
* CPU is allocated to the processes for the limited time
* executing process can be interrupted, if higher priority one comes

**CPU scheduler**
* selects process to be executed and allocate CPU to it

**Dispatcher**
* is the module that gives control of the CPU ’s core to the process selected by the CPU scheduler
* This function involves the following:
    * Switching context from one process to another
    * Switching to user mode
    * Jumping to the proper location in the user program to resume that program
* dispatcher should be as fast as possible, since it is invoked during every context switch
* The time it takes for the dispatcher to stop one process and start another running is known as the **dispatch latency**

```bash
# to know number of context switches
$ vmstat
vmstat 1 3
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 3428216 191232 2028656    0    0    37    23   96  308  3  1 96  0  0
 0  0      0 3428216 191232 2028656    0    0     0    44  223  529  0  0 99  0  0
 0  0      0 3428216 191232 2028656    0    0     0     0  225  530  0  0 99  0  0

# 308 context switches over 1 second since the system booted
# 529 context switchers in past 1 second
# 530 context switches in second prior to it

$ cat /proc/2362/status | grep ctxt_switches
voluntary_ctxt_switches:	245372
nonvoluntary_ctxt_switches:	4897

# shows the number of context switches over the lifetime of the process
```
**voluntary context**
* switch occurs when a process has given up control of the CPU because it requires a resource that is currently unavailable (such as blocking for I/O .)

**nonvoluntary context**
* switch occurs when the CPU has been taken away from a process, such as when its time slice has expired or it has been preempted by a higher-priority process

---
**Scheduling Criteria:**
* CPU utilization
    * nedd is to keep the CPU as busy as possible
* Throughput
    * number of processes that are completed per time unit
* Turnaround time
    * interval from the time of submission of a process to the time of completion
* Waiting time
    * sum of the periods spent waiting in the ready queue
* Response time
    * time from the submission of a request until the first response is produced

---

Scheduling Algorithms ( in context of single core single CPU)
* First-Come, First-Served Scheduling
    * average waiting time is quite long
    * CPU may be idle waiting for processes to finish I/O
    * I/O devices may be idle waiting for process to finish CPU
    * convoy effect can take place as
        * all the other processes may wait for the one big process to get off the CPU

* Shortest-Job-First Scheduling
    * there is no way to know the length of the next CPU burst
    * next CPU burst may be predicted as an exponential average of the measured lengths of previous CPU bursts
    * gives the minimum average waiting time for a given set of processes
    * high CPU usage process may wait forever

* Round-Robin Scheduling
    * small unit of time, called a time quantum or time slice, is defined
    * time quantum is generally from 10 to 100 milliseconds in length
    * ready queue is treated as a circular queue
    * CPU scheduler goes around the ready queue, allocating the CPU to each process for a time interval of up to 1 time quantum
    * performance depends heavily on the size of the time quantum
        * if time quanta is large , RR will become FCFS
        * if time quanta is small, large context switches will happen
        * rule of thumb is that 80% of the CPU bursts should be shorter than the time quantum

* Priority Scheduling
    * priority is associated with each process
    * CPU is allocated to the process with the highest priority
    * same priority process can be scheduled in FCFS order / round robin order
    * priorities can be defined either internally or externally
    * internally defined priorities use some measurable quantity or quantities to compute the priority of a process
        * e.g. time limits, memory requirements, the number of open files, the ratio of average I/O burst to average CPU burst
    * problem:
        * indefinit blociing, or starvation
        * some lowpriority processes can wait indefinitely
    * solution:
        * aging
            * gradually increasing the priority of processes that wait in the system for a long time
            * e.g.
                * priortity can be increased by 1 for every seond passed

* Multilevel Queue Scheduling
    * instead of one queue, mutiple queues are used based on priority
    * processes with same priority are place in same queue
    * it helps in reducing the length of queue and hence reducing O(n) complexiety for round robin selection
    * can also be used to partition processes into several separate queues based on the process type, e.g.
        * Real-time processes
        * System processes
        * Interactive processes
        * Batch processes
    * each queue may have its own scheduling algorithm
    * there must be scheduling among the queues
        * as fixed-priority preemptive scheduling or
        * time-slicinf among the queues
            * each queue gets a certain portion of the CPU time, which it can then schedule among its various processes
            * e.g.
                * in the foreground-background queue example
                * the foreground queue can be given 80 percent of the CPU time
                * and background queue - 20 % of CPU time

* Multilevel Feedback Queue Scheduling
    * allows a process to move between queues
    * idea is to separate processes according to the characteristics of their CPU bursts
        * process using too much CPU time, will be moved to a lower-priority queue
        * process that waits too long in a lower-priority queue may be moved to a higher-priority queue
        * it prevents starvation

---

Multiple-Processor Scheduling

* standard approach for supporting multiprocessors is symmetric multiprocessing (SMP)  where each processor is self-scheduling

    * All threads may be in a common ready queue or
        * may have race condition, must ensure that
            * not two processors choose same thread
            * threads are not lost from the queue
        * locking can be used, but it will become bottleneck

    * Each processor may have its own private queue of threads
        * common approach on most of the system
        * balancing algorithms can be used to equalize workloads among all processors


---

Multicore Processors

* each core maintains its architectural state and thus appears to the OS to be a separate logical CPU

Problem:
* when a processor accesses memory, it spends a significant amount of time waiting for the data to become available known as a **memory stall**, occurs because
    * processors operate at much faster speeds than memory
    * cache miss (accessing data that are not in cache memory)
    * processor can spend up to 50% of its time waiting for data to become available from memory

Solution:
* **multithreaded processing cores**
    * in which two (or more) hardware threads are assigned to each core
    * if one hardware thread stalls while waiting for memory, the core can switch to another thread

* From an OS perspective, each hardware thread maintains its architectural state, such as instruction pointer and register set, and thus appears as a logical CPU that is available to run a software thread. This technique is known as **chip multithreading (CMT)**

* Intel processors use the term **hyper-threading** (simultaneous threading SMT)

> Oracle Sparxc M7 processor supports 8 threads per core, with eight cores per processor, thus providing OS with 64 logical CPUs

multithreaded, multicore processor actually requires two different levels of
scheduling,
* one level by OS to choose which process to run on each hardware thread(logical CPU)
* second level by core to decide which hardware thread to run

two levels are not necessarily mutually exclusive
* if OS is aware of sharing of processor resources, it can make more effective scheduling decisions, like instead of assigning same core(two hardware threads) to two threads it can assign different cores to each thread

```
chip multithreading
 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
|               processor                   |
| ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁  |
||      core0      |    |     core1       | |
|| hardware thread |    | hardware thread | |
|| hardware thread |    | hardware thread | |
| ▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔      ▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔  |
|                                           |
| ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁  |
||      core2      |    |     core3       | |
|| hardware thread |    | hardware thread | |
|| hardware thread |    | hardware thread | |
| ▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔      ▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔▔  |
|▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁|

 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
|                  OS view                  |
|  ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁ |
| |       |  |       |  |       |  |       ||
| | core0 |  | core1 |  | core2 |  | core3 ||
| |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁||
|                                           |
|  ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁    ▁▁▁▁▁▁▁ |
| |       |  |       |  |       |  |       ||
| | core4 |  | core5 |  | core6 |  | core7 ||
| |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁|  |▁▁▁▁▁▁▁||
|▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁|

```
---

Load balancing
* Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system
* load balancing is typically necessary only on systems where each processor has its own private ready queue of eligible threads to execute
* On systems with a common run queue, load balancing is unnecessary, because once a processor becomes idle, it immediately extracts a runnable thread from the common ready queue

two general approaches to load balancing

* push migration
    * specific task periodically checks the load on each processor
    * if it finds an imbalance
        * it evenly distributes the load by moving tasks from overloaded to less busy processors

* pull migration
    * idle processor pulls a waiting task from a busy processor

* Push and pull migration need not be mutually exclusive
    * in fact, often implemented in parallel on load-balancing systems
    * e.g.
        * Linux CFS scheduler implements both technique

concept of a "balanced load" may have different meanings
* same number of tasks per queue
* equal distribution of task priorties across all queues, so on

---

**Processor Affinity**

* data most recently accessed by the thread populate the cache for the processor
* As a result, successive memory accesses by the thread are often satisfied in cache memory (known as a **warm cache**)
* moving of task from one core to another will result in
    * cache memory invalidation on first core
    * repopulation of cache of second core
* due to high cost of invalidating and repopulating caches, most OS with SMP support try to avoid migrating a thread from one processor to another
* This is known as **processor affinity**
    * i.e. process has an affinity for the processor on which it is currently running

**soft affinity**
* OS attempts to keep a process running on the same processor but not guaranteeing that it will do so

**hard affinity**
* guarantee to run process on specified cores only

> Linux provides soft affinity, and also provides `sched setaffinity()` system call, which supports hard affinity by allowing a thread to specify the set of CPUs on which it is eligible to run


If OS is aware of NUMA architecture
* then a thread that has been scheduled onto a particular CPU can be allocated memory closest to where the CPU resides

> there is a natural tension between load balancing and minimizing memory access times

---

Real-Time CPU Scheduling

Soft real-time systems
* provide no guarantee as to when a critical real-time process will be scheduled
* guarantee only that the process will be given preference over noncritical processes

Hard real-time systems
* a task must be serviced by its deadline
    * service after the deadline has expired is the same as no service at all

event latency
* amount of time that elapses from when an event occurs to when it is serviced

different events have different latency requirements

Two types of latencies affect the performance of real-time systems:
1. Interrupt latency
    * period of time from the arrival of an interrupt at the CPU to the start of the routine that services the interrupt
    * one important factor contributing to interrupt latency is the amount
of time interrupts may be disabled while kernel data structures are being
updated
        * requirement is that interrupts be disabled for only very short period of time

2. Dispatch latency
    * amount of time required for the scheduling dispatcher to stop one process and start another
    * conflict phase of dispatch latency has two components:
        1. Preemption of any process running in the kernel
        2. Release by low-priority processes of resources needed by a high-priority process

---

**Completely Fair Scheduler ( CFS )** ???
* in release 2.6.23 of the kernel, became the default Linux scheduling algorithm

Scheduling in the Linux system is based on scheduling classes
* Each class is assigned a specific priority
* By using different scheduling classes, the kernel can accommodate different scheduling algorithms based on the needs of the system and its processes
* Standard Linux kernels implement two scheduling classes:
    * a default scheduling class using the CFS scheduling algorithm
    * a real-time scheduling class
* new scheduling classes can be added
* proportion of CPU is calculated based on the nice value assigned to each task
    * Nice values range from −20 to +19
        * lower values means high priority
        * default nice value is 0