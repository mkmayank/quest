*1  Kubernetes cluster consists machines ( virtual, physical any)
        1.  Master  -   one machine is Master       -   manage the cluster
        2.  Nodes   -   other machines are nodes    -   workers that run applications

        ***************************
        Master
            manages actvities like :
                scheduling applications
                maintaining applications' desired state
                scaling applications
                rolling out new updates, ...

        Node
            each node has to has
                1.  Kubelet -   agent for managing the node and communicating with the K8s master
                2.  container application   -   such as Docker or rkt

        ***************************
    layman :
    kubernets machines kaa cluster hotaa hai, ek machine master hoti hai , or baaki sab nodes hotey hai
    master pure cluster ko manager kartaa hai, or hum iss ko command kar saktey hai kuch kar waane ke liye
    node pe kubelet installed honaa chaiye jo node pe containers chawaayegaa or master se baat karegaa
    node pe container tool bhi honaa chaye, jise kubernets use karegaa
    master se baat karney ke liye command line tool hai kuberctl

=====================

*2  Processes :
        Master is a collection of three processess :
            1.  kube-apiserver          -   validates and configures data for the api objects ( pods, services, ...)
                                            services REST operations and let components interact to it

            2.  kube-controller-manager -   daemon that embeds the core control loops shipped with Kubernetes

            3.  kube-scheduler          -   watches newly created pods that have no node assigned, and selects a node for them to run on

        Non-master nodes runs two processess :
            1.  kubelet     -   communicates with the Kubernetes Master
                                works in terms of a PodSpec
                                ensures that the containers described in PodSpecs are running and healthy

            2.  kube-proxy  -   enables the Kubernetes service abstraction
                                maintains network rules on the host and performs connection forwarding

        ***************************

    -   Typically master processes are all run on a single machine and do not run user containers on same machine
        The master can also be replicated for availability and redundancy

    -   controller  -   control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state
                        e.g.    replication controller, endpoints controller, namespace controller, and serviceaccounts controller

    -   PodSpec     -   a YAML or JSON object that describes a pod

    -   There are 4 ways that a container manifest can be provided to the Kubelet
            1.  ApiServer:      from an PodSpec from the apiserver

            2.  File:           Path passed as a flag on the command line
                                monitored periodically for updates ( 20s default )

            3.  HTTP endpoint:  HTTP endpoint passed as a parameter on the command line
                                monitored periodically for updates ( 20s default )

            4.  HTTP server:    listen for HTTP and respond to a simple API to submit a new manifest

=====================

*3  basic Kubernetes objects include:
        Pod
        Service
        Volume
        Namespace

    Controllers :

        ReplicaSet
        Deployment
        StatefulSet
        DaemonSet
        Job

=====================

*4  Components
        Master :
            1.  kube-apiserver

            2.  etcd                    -   Consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data
                                        -   https://github.com/coreos/etcd/blob/master/Documentation/docs.md

            3.  kube-scheduler

            4.  kube-controller-manager -   runs controllers

            5.  cloud-controller-manager-   runs controllers that interact with the underlying cloud providers
                                        -   runs cloud-provider-specific controller loops only

        Node :
            1.  kubelet
            2.  kube-proxy
            3.  container runtime

        Addons :    extend the functionality of Kubernetes
            1.  DNS
            2.  Web UI (Dashboard)
            3.  Container Resource Monitoring
            4.  Cluster level logging

        ***************************

    -   Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.

    -   kube Controllers include:

            1.  Node Controller                     :   Responsible for noticing and responding when nodes go down

            2.  Replication Controller              :   Responsible for maintaining the correct number of pods for every replication controller object in the system

            3.  Endpoints Controller                : Populates the Endpoints object (joins Services & Pods)

            4.  Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces

    -   cloud-controller-manager allows cloud vendors code and the Kubernetes core to evolve being independent to each other

=====================

*5. Objects
        -   persistent entities in the Kubernetes system ???
        -   represent the state of cluster
        -   describe :
            -   what containerized applications are running
            -   resources available to applications
            -   policies around how applications behave, ( restart, upgrades, fault-tolerance)

        -   Every Kubernetes object includes two nested object fields that govern the object’s configuration:
                1.  the object spec     -   describes the desired state for the object
                                        -   provided by user
                2.  the object status   -   describes the actual state of the object
                                        -   supplied and updated by the Kubernetes system

            At any given time, the Kubernetes Control Plane actively manages an object’s actual state to match the desired state

        -   Creation of object :
                -   provide the object spec as well as some basic information about the object (such as a name)
                -   API expects spec in JSON in the request body
                -   kubectl accepts spec in a YAML or JSON format, converts YAML to JSON when making the API request


        -   .yaml file
                -   apiVersion      -   version of the Kubernetes API
                -   kind            -   kind of object to create
                -   metadata        -   Data that helps uniquely identify the object, including a name, UID, and optional namespace
                -   spec

                e.g.
                    apiVersion: apps/v1
                    kind: Deployment
                    metadata:
                      name: nginx-deployment
                    spec:
                        ...

                $ kubectl create -f file.yaml --record

        -   All objects in the Kubernetes REST API are unambiguously identified by a Name and a UID
                1.  Name
                    -   user provides it
                    -   conventions to have lower case alphanumeric characters, -, .
                2.  UID
                    -   K8s generates this
                    -   distinct for every object over whole lifetime of a cluster
                        intended to distinguish between historical occurrences of similar entities

        -   Namespace
                -   K8s supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces
                -   Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.

                        $   kubectl get namespaces

                -   Kubernetes starts with 3 initial namespaces:
                        1.  default     -   default namespace for objects with no other namespace
                        2.  kube-system -   for objects created by the Kubernetes system
                        3.  kube-public -   readable by all users, reserved for cluster usage, in case some resources should be visible and readable publicly throughout the whole cluster

                -   Setting the namespace for a request

                    -   temporarily

                        $   kubectl --namespace=<insert-namespace-name-here> run nginx --image=nginx
                        $   kubectl --namespace=<insert-namespace-name-here> get pods

                    -   permanently for all subsequent kubectl commands in the context

                        $   kubectl config set-context $(kubectl config current-context) --namespace=<insert-namespace-name-here>

        -   Labels and Selectors
                -   key/value pairs (user-defined) attached to objects
                -   meaningful and relevant to users only and not to core system
                -   used to organize and to select subsets of objects
                -   can be attached to objects at creation time and subsequently added and modified at any time

                    e.g.
                    Example labels:
                        "release" : "stable", "release" : "canary"
                        "environment" : "dev", "environment" : "qa", "environment" : "production"
                        "tier" : "frontend", "tier" : "backend", "tier" : "cache"
                        "partition" : "customerA", "partition" : "customerB"
                        "track" : "daily", "track" : "weekly

                -   2 types of selectors
                        1.  equality-based  -   Operators =,==,!=
                                e.g.
                                environment = production
                                tier != frontend

                        2.  set-based       -   Operators in, notin, exists ( only the key identifier )
                                e.g.
                                environment in (production, qa)
                                tier notin (frontend, backend)
                                partition,environment notin (qa)

                        selectos can be mixed , like :
                            partition in (customerA, customerB),environment!=qa

                -   A label selector can be made of multiple requirements which are comma-separated ( && )

                        e.g.

                            $   kubectl get pods -l environment=production,tier=frontend
                            $   kubectl get pods -l 'environment in (production),tier in (frontend)'

                -   In YAML
                        e.g.
                            selector:
                                component: redis

            -   Annotations
                    -   to attach arbitrary non-identifying metadata to objects
                    -   annotations are not used to identify and select objects
                    -   Annotations, like labels, are key/value maps:

                            "annotations": {
                              "key1" : "value1",
                              "key2" : "value2"
                            }


        ***************************

    layman status spec example:
    agar humne deploymnet object banaya jisme humne 3 replica set kara , ye spec hai, K8s iss spec ko padegaa or 3 instance start karegaa taki status match kar sakey, agar kisi wajah se koi intanace down ho jaata hai, to uskaa status me replica 2 ho jaayegi to K8s fir se ek start karegaa taaki status ko spec se match kar sakey

    layman namespace:
    namespace se hum object baat saktey hai , taaki sirf ek namespace waale hee object aapas me baat kar sakey or naa kii dusrey namespace se, agar ek cluster me 2 yaa or differnet cheezey hai to namespace se unhe alag kiyaa jaa saktaa hai, jaise mutiple teams kaam kar rai hai ek cluster me to quota banane ke liye namespace bnaaye jaa saktey hai

    layman labels:
    labels objects kee grouping yaa usko kuch hamaari apni convetins dene ke liye hai, jaise kuch objects ko hum env:dev de saktey hai or kuch ko env:prod or fir dekh saktey hai kiii konsaa dev hai or konse prod

=====================

*6  Management
        -   Kubernetes object should be managed using only one technique
            Mixing and matching techniques for the same object results in undefined behavior

        Techniques
            1.  Imperative commands ( sidhe command line pe bolke bnaawaana )
                -   operates directly on live objects in a cluster
                -   user provides operations to the kubectl command as arguments or flags.
                -   simplest way
                -   provides no history of previous configurations

                    e.g.
                        kubectl run nginx --image nginx
                        kubectl create deployment nginx --image nginx

            2.  Imperative object configuration
                -   using file containing definition of the object in YAML or JSON format
                -   Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement

                    e.g.
                        kubectl create -f nginx.yaml
                        kubectl delete -f nginx.yaml -f redis.yaml

            3.   Declarative object configuration
                -   object configuration files stored locally, however the user does not define the operations to be taken on the files
                -   Changes made directly to live objects are retained, even if they are not merged back into the configuration files

                    e.g.
                        kubectl apply -f configs/

        ***************************
        -   Imperative commands
                https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/

        -   Imperative object configuration
                https://kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/

        -   Declarative object configuration
                https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/

        -   Export the live object to a local object configuration file:

                $   kubectl get <kind>/<name> -o yaml --export > <kind>_<name>.yaml

=====================

*7. Node

        -   Node Status
                1.  Addresses
                    1.  HostName
                    2.  ExternalIP
                    3.  InternalIP

                2.  Condition
                    1.  OutOfDisk           -   if free space on the node for adding new pods
                    2.  Ready               -   if node is healthy and ready to accept pods
                    3.  MemoryPressure	    -   if the node memory is low
                    4.  DiskPressure        -   if the disk capacity is low
                    5.  NetworkUnavailable	-   if the network for the node is correctly configured
                    6.  ConfigOK            -	if the kubelet is correctly configured

                3.  Capacity
                    1.  CPU
                    2.  memory
                    3.  max number of pods that can be scheduled onto the node

                4.  Info
                    general info, like kernel version, K8s version, docker version, OS name, ...

        -   when Kubernetes creates a node, it is just creating an object that represents the node
            After creation, Kubernetes will check whether the node is valid or not

        -   3 components to interact with node :
                1.  Node controller
                    -   K8s master component
                    -   roles
                        1.  assigns CIDR ( Classless Inter-Domain Routing ) (if turned on) when node registered
                        2.  keeping the node controller’s internal list of nodes up to date with the cloud provider’s list of available machines
                        3.  monitoring the node's health
                2.  kubectl
                3.  kubelet

        -   Self registration of nodes
                -   kubelet attempt to register itself with the API server
                -   kubelet options to self registered
                    --register-node ( true by default)
                    --kubeconfig
                    --cloud-provider
                    --node-ip
                    --node-lables
                    --node-status-update-frequency

        -   Marking a node as unschedulable will prevent new pods from being scheduled to that node, but will not affect any existing pods on the node
            This is useful as a preparatory step before a node reboot, etc

                $   kubectl cordon $NODENAME        #   to mark node unschedulable


        ***************************

        -   If Status of the Ready condition is “Unknown” or “False” for longer than the pod-eviction-timeout (default 5 mins)
            then all of the Pods on the node are scheduled for deletion by the Node Controller
            If not reachable to node, the pods which are scheduled for deletion may continue to run on the partitioned node

        -   When running in a cloud environment, whenever a node is unhealthy, the node controller asks the cloud provider if the VM for that node is still available. If not, the node controller deletes the node from its list of nodes.

        -   any kubelet is authorized to create/modify any node resource, but in practice it only creates/modifies its own

=====================

*8  Communication
        1.  Node to Master ( apiserver )
            -   listens via HTTPS with some authorization
            -   secured

        2.  Master ( apiserver ) to Node
            -   unsecured
                1.  apiserver to kubelet
                2.  apiserver to node, pod, or service

        ***************************

=====================

*9  Cloud controller manager
        -   single point of integration with the cloud
        -   Controllers having cloud provider dependencies:

                1.  Node Controller
                2.  Route Controller
                3.  Service Controller
                4.  Volume Controller   : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes

        -   CCM runs
                1.  Node controller                     :   initialize node by obtaining information about the nodes running in the cluster from the cloud provider
                                                            In case a node becomes unresponsive, check the cloud to see if the node has been deleted from the cloud. If the node has been deleted from the cloud, delete the Kubernetes Node object

                                                            API access : node   ->  get,list,create,update,patch,watch,delete

                2.  Route controller                    :   configures routes in the cloud so that containers on different nodes can communicate with each other

                                                            API access : node   ->  get

                3.  Service controller                  :   listens to service create, update, and delete events

                                                            API access : service    ->  create,list,get,watch,patch,update

                4.  PersistentVolumeLabels controller   :   responsible for setting the zone and region labels on PersistentVolumes created in GCP and AWS clouds

                                                            API access : PersistentVolume   ->  get,list,watch,update

                Core
                    API access  :   Event   ->  Create,Patch,Update
                                    ServiceAccount  ->  Create

                Volume Controller was deliberatley chosen to not be a part of CCM due to complexity

        ***************************

        -   now cloud vendor's responsiblity to create or update CCM, k8s pe paisaa hai naa

=====================

*10 Containers
    -   default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists
        to always pull
            1.  set the imagePullPolicy of the container to Always
            2.  or  use :latest as the tag for the image to use
            3.  or  enable the AlwaysPullImages admission controller

    -   Variables available to containers :
        -   hostname of a Container is the name of the Pod in which the Container is running
        -   User defined environment variables from the Pod definition are available to the Container, as are any environment variables specified statically in the Docker image
        -   list of all services that were running when a Container was created is available to that Container as environment variables

    -   Container lifecycle hooks :
            1.  PostStart       -   executes immediately after a container is created

            2.  PreStop         -   called immediately before a container is terminated

            Hook handler implementations :
                1.  Exec    - Executes inside the cgroups and namespaces of the Container

                2.  HTTP    - Executes an HTTP request against a specific endpoint on the Container

            When a Container lifecycle management hook is called, the K8s management system executes the handler in the Container registered for that hook

                -   Hook handler calls are synchronous within the context of the Pod containing the Container
                    i.e.
                        PostStart hook, the Container ENTRYPOINT and hook fire asynchronously. However, if the hook takes too long to run or hangs, the Container cannot reach a running state.
                        if PreStop Hook hangs during execution, the Pod phase stays in a Terminating state and is killed after terminationGracePeriodSeconds of pod ends.
                -   If a PostStart or PreStop hook fails, it kills the Container.

                -   hook handlers should be as lightweight as possible

            logs for a Hook handler are not exposed in Pod events
            If a handler fails for some reason, it broadcasts an event
                -   PostStart   ->  FailedPostStartHook event
                -   PreStop     ->  FailedPreStopHook event

            see events by running
                $    kubectl describe pod <pod_name>

        ***************************

=====================

11* Pod
    -   smallest and simplest unit in the Kubernetes object model
    -   Each Pod is meant to run a single instance of a given application
        to run multiple instances, should use multiple Pods, one for each instance, generally referred to as replication
        Replicated Pods are usually created and managed as a group by an abstraction called a Controller

    Shared resources for constituent containers:
        1.  Networking
                -   Each Pod is assigned a unique IP address
                -   Containers within a pod share an IP address and port space, and can find each other via localhost
                -   Containers within a pod can communicate to each other using IPC

        2.  Storage
                -   All containers in the Pod can access the shared volumes, allowing those containers to share data
                -   Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be restarted

    Pod phase :
        1.  Pending     :   Pod has been accepted by the Kubernetes system, but one or more of the Container images has not been created
                            includes time before being scheduled as well as time spent downloading images over the network

        2.  Running     :   Pod has been bound to a node, and all of the Containers have been created
                            At least one Container is still running, or is in the process of starting or restarting

        3.  Succeeded   :   All Containers in the Pod have terminated in success, and will not be restarted

        4.  Failed      :   All Containers in the Pod have terminated, and at least one Container has terminated in failure
                            i.e. the Container either exited with non-zero status or was terminated by the system

        5.  Unknown     :   For some reason the state of the Pod could not be obtained

    -   Pods with a phase of Succeeded or Failed for more than some duration (determined by the master) will expire and be automatically destroyed
    -   once bound to a node, a Pod will never be rebound to another node

    Pod conditions :
        type:status
        type :
            1.  PodScheduled
            2.  Ready
            3.  Initialized
            4.  Unschedulable
        status :
            True, False, and Unknown

    Probe :
        -   describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic
        -   performed periodically by the kubelet on a Container

        -   To perform a diagnostic, the kubelet calls a Handler implemented by the Container
                1.  ExecAction      :   Executes command inside the Container
                                        successful  ->  exit status code 0

                2.  TCPSocketAction :   Performs a TCP check against the Container’s IP address on a specified port
                                        successful  ->  port is open

                3.  HTTPGetAction   :   Performs an HTTP Get request against the Container’s IP address on a specified port and path
                                        successful  ->  status code is 2XX or 3XX

        -   2 kinds of probes :
                1.  livenessProbe   :   Indicates whether the Container is running
                                        fail    ->  kubelet kills the Container, and the Container is subjected to its restart policy
                                        If a Container does not provide a liveness probe, the default state is Success

                2.  readinessProbe  :   Indicates whether the Container is ready to service requests
                                        fails   ->  endpoints controller removes the Pod’s IP address from the endpoints of all Services that match the Pod
                                        The default state of readiness before the initial delay is Failure
                                        If a Container does not provide a readiness probe, the default state is Success

    Restart policy :
        1.  Always      -   default
        2.  OnFailure
        3.  Never
        -   restartPolicy only refers to restarts of the Containers by the kubelet on the same node
        -   Failed Containers are restarted with an exponential back-off delay (10s, 20s, 40s …) capped at 5 minutes, and is reset after ten minutes of successful execution

    Controllers to choose
        -   Pods that are expected to terminate         ->  Job
        -   Pods that are not expected to terminate     ->  ReplicationController, ReplicaSet, Deployment
        -   Pods that need to run one per machine       ->  DaemonSet

    Init Containers
        -   which are run before the app Containers are started
            Init Containers are exactly like regular Containers, except:
                1.  They always run to completion
                2.  Each one must complete successfully before the next one is started

        -   If an Init Container fails for a Pod, Kubernetes restarts the Pod repeatedly until the Init Container succeeds
        -   Init Containers do not support readiness probes because they must run to completion before the Pod can be ready

    Pod Preset
        -   an API resource for injecting additional runtime requirements into a Pod at creation time
        -   No resource definition from the Pod Preset will be applied to the initContainers field
        -   K8s provides an admission controller (PodPreset) which, when enabled, applies Pod Presets to incoming pod creation requests

            When a pod creation request occurs, the system does the following:
                1.  Retrieve all PodPresets available for use
                2.  Check if the label selectors of any PodPreset matches the labels on the pod being created.
                3.  Attempt to merge the various resources defined by the PodPreset into the Pod being created.
                    On error, throw an event documenting the merge error on the pod, and create the pod without any injected resources from the PodPreset.
                4.  Annotate the resulting modified Pod spec to indicate that it has been modified by a PodPreset
                    The annotation is of the form
                        podpreset.admission.kubernetes.io/podpreset-<pod-preset name>: "<resource version>"

        -   Disable Pod Preset for a specific pod
                add an annotation in the Pod Spec of the form
                    podpreset.admission.kubernetes.io/exclude: "true"

        -   Enable Pod Preset
                1.  enable the API type settings.k8s.io/v1alpha1/podpreset
                        include settings.k8s.io/v1alpha1=true in the --runtime-config option for the API server

                2.  enable the admission controller PodPreset
                        include PodPreset in the --enable-admission-plugins option value specified for the API server

                3.  defined Pod Presets

    PodDisruptionBudget object (PDB)
        -   limits the number pods of a replicated application that are down simultaneously from voluntary disruptions
                e.g.
                    a Deployment which has a spec.replicas: 5 is supposed to have 5 pods at any given time
                    If its PDB allows for there to be 4 at a time,
                    then the Eviction API will allow voluntary disruption of one, but not two pods, at a time
        -   PDB cannot prevent involuntary disruptions ( i.e. general technical problems)
            PDB helps in preventing admin manual accidents, like admin deleting some node,pod,.. without checking if it can be done
        -   When a pod is evicted using the eviction API, it is gracefully terminated

        ***************************

    -   Layman
            -   K8s containers ke saath direct deal naa karke pod object ke through deal kartaa hai, ek pod me ek yaa jaada containers ho saktey h
            -   multiple pods ko manage karney ke liye controllers hotey h

    -   Layman Pod preset
            -    this is like humaaraa chef se configuration files me entry karnaa jo run time pe hee avialble hoti hai, jaise ips of some services depending on environment

=====================

12* 

        ***************************

=====================
